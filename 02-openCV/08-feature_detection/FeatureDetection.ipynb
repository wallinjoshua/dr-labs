{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST import all the necessary libraries and modules!\n",
    "import cv2               # import OpenCV\n",
    "import numpy as np       # import NumPy\n",
    "\n",
    "# import instructor made functions \n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from utils import *      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Detection Lab\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Let's create a program to <b style=\"color:magenta\">track an object</b> (ie. Traffic Signs) using its <b style=\"color:magenta\">features</b> in a video!\n",
    "    </p> \n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    In this lab, we will learn how to: \n",
    "    <ul style='font-size:1.75rem;line-height:2'>\n",
    "        <li><b style=\"color:blue\">DRAW features</b> on images:\n",
    "            <br> <code>cv2.drawKeypoints</code></li>\n",
    "        <li><b style=\"color:green\">DETECT features</b> of an image using <b style=\"color:green\">three different</b> feature detection algorithms: \n",
    "            <br> <code>SIFT</code>, <code>SURF</code>, and <code>ORB</code></li>\n",
    "        <li><b style=\"color:orange\">FIND matches</b> between images: \n",
    "            <br><code>FLANN</code></li>\n",
    "    </ul>\n",
    "    </p> \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Features?\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:blue\">Features (keypoints)</b> are <b style=\"color:blue\">distinct and easily recognizable</b> regions of an image. \n",
    "    </p>\n",
    "\n",
    "## Drawing Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To <b style=\"color:blue\">draw keypoints</b> on an image, use <code>cv2.drawKeypoints</code>. \n",
    "    <br>It has the following format:\n",
    "    </p>\n",
    "\n",
    "```python\n",
    "cv2.drawKeypoints(<image>, <keypoints>, <image>, flags=5)\n",
    "```\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Here is an example of keypoints (the circles) drawn on an image of a building. \n",
    "    </p>\n",
    "\n",
    "<img src=\"building_keypoints.jpg\" alt=\"building_keypoints\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    We will be introducing <b style=\"color:green\">three different</b> feature detection functions:\n",
    "    <br><code>SIFT</code>, <code>SURF</code>, and <code>ORB</code>\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    For more information on SIFT, SURF, and ORB, check out the article below!\n",
    "    </p>\n",
    "\n",
    "https://pysource.com/2018/03/21/feature-detection-sift-surf-obr-opencv-3-4-with-python-3-tutorial-25/\n",
    "\n",
    "\n",
    "### SIFT\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    SIFT can detect features in an image <b style=\"color:green\">regardless of size or scale</b>.\n",
    "    </p>\n",
    "    \n",
    "<img src=\"TaylorSIFT.jpg\" alt=\"TaylorSIFT\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To get an <b style=\"color:green\">image's features using SIFT</b>, use <code>sift.detectAndCompute</code>. \n",
    "    <br> This function can only take in <b style=\"color:green\">grayscale images</b>.\n",
    "    </p> \n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p> \n",
    "\n",
    "```python\n",
    "keypoints, descriptors = sift.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Get the <b>features</b> from <code>building.jpg</code> using <b>SIFT</b>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK #1: Read \"building.jpg\"\n",
    "\n",
    "\n",
    "# TASK #2: Convert BGR image to grayscale via cv2.cvtColor\n",
    "\n",
    "\n",
    "# Create a SIFT object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# TASK #3: Get keypoints of the grayscale image via sift.detectAndCompute\n",
    "\n",
    "\n",
    "# TASK #4: Draw the keypoints onto the image via cv2.drawKeypoints\n",
    "\n",
    "\n",
    "# TASK #5: Show the image via cv2.imshow\n",
    "\n",
    "\n",
    "# TASK #6: Close the window\n",
    "\n",
    "\n",
    "# Print the number of keypoints detected!\n",
    "print(\"# SIFT Keypoints: {}\".format(len(keypoints)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    SURF detects features <b style=\"color:magenta\">faster than SIFT</b>\n",
    "    </p>\n",
    "    \n",
    "<img src=\"BetterSurf.png\" alt=\"BetterSurf\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To get an <b style=\"color:magenta\">image's features using SURF</b>, there are two steps: \n",
    "    <ul style='font-size:1.75rem;line-height:1.5'>\n",
    "      <li>Set <code>hessianThreshold</code> to <b style=\"color:green\">control the number of features found</b> in an image.\n",
    "          <br> <b style=\"color:green\">Threshold values</b> are usually between <code>300</code> to <code>500</code>. \n",
    "          <br><b style=\"color:green\">Smaller thresholds create more keypoints</b>.</li>\n",
    "        <br>\n",
    "        <li><b style=\"color:blue\">Get SURF features</b> using <code>surf.detectAndCompute</code>. \n",
    "            <br>This function can only take in <b style=\"color:blue\">grayscale images</b>. </li>\n",
    "     </ul>\n",
    "     </p>\n",
    "     \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p> \n",
    "\n",
    "```python\n",
    "keypoints, descriptors = surf.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Get the <b>features</b> from <code>building.jpg</code> using <b>SURF</b>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK #1: Read \"building.jpg\"\n",
    "\n",
    "\n",
    "# TASK #2: Convert BGR image to grayscale via cv2.cvtColor\n",
    "\n",
    "\n",
    "# TASK #3: Set hessianThreshold to your desired value (bigger = less keypoints)\n",
    "hessianThreshold = None       # Change this number!\n",
    "\n",
    "# Create a SURF object\n",
    "surf = cv2.xfeatures2d.SURF_create(hessianThreshold, extended=True)\n",
    "\n",
    "# TASK #4: Get keypoints of the grayscale image via surf.detectAndCompute\n",
    "\n",
    "\n",
    "# TASK #5: Draw the keypoints onto the image via cv2.drawKeypoints\n",
    "\n",
    "\n",
    "# TASK #6: Show the image via cv2.imshow\n",
    "\n",
    "\n",
    "# TASK #7: Close the window\n",
    "\n",
    "\n",
    "# Print the number of keypoints detected, and the hessian_threshold\n",
    "print(\"# SURF Keypoints: {}, hessianThreshold = {}\".format(len(keypoints), hessianThreshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    ORB detects <b style=\"color:orange\">less features</b> than SIFT and SURF, but <b style=\"color:orange\">ORB is faster</b>.\n",
    "    </p>\n",
    "    \n",
    "<img src=\"Orbz.png\" alt=\"Orbz\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    To get an <b style=\"color:orange\">image's features using ORB</b>, there are two steps: \n",
    "    <ul style='font-size:1.75rem;line-height:1.5'>\n",
    "      <li>Set <code>nFeatures</code> to <b style=\"color:green\">set the maximum number of features found</b> in the image.\n",
    "          <br> The <b style=\"color:green\">default value</b> is <code>500</code></li>\n",
    "        <br>\n",
    "        <li><b style=\"color:blue\">Get ORB features</b> using <code>orb.detectAndCompute</code>. \n",
    "            <br>This function can only take in <b style=\"color:blue\">grayscale images</b>. </li>\n",
    "     </ul>\n",
    "     </p>\n",
    "     \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p> \n",
    "\n",
    "```python\n",
    "keypoints, descriptors = orb.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Get the <b>features</b> from <code>building.jpg</code> using <b>ORB</b>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK #1: Read \"building.jpg\"\n",
    "\n",
    "\n",
    "# TASK #2: Convert BGR image to grayscale via cv2.cvtColor\n",
    "\n",
    "\n",
    "# TASK #3: Set the maximum number of features detected via nFeature\n",
    "nFeatures = None       # Change this number!\n",
    "\n",
    "# Create an ORB object\n",
    "orb = cv2.ORB_create(nFeatures)\n",
    "\n",
    "# TASK #4: Get keypoints of the grayscale image via orb.detectAndCompute\n",
    "\n",
    "\n",
    "# TASK #5: Draw the keypoints onto the image via cv2.drawKeypoints\n",
    "\n",
    "\n",
    "# TASK #6: Show the image via cv2.imshow\n",
    "\n",
    "\n",
    "# TASK #7: Close the window\n",
    "\n",
    "\n",
    "# Print the number of keypoints detected, and the nFeatures\n",
    "print(\"# ORB Keypoints: {}, nFeatures = {}\".format(len(keypoints), nFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Traffic Signs\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Let's create a program that will <b style=\"color:green\">detect and track a one way sign</b> in a live video stream.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parameters\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Set the <b style=\"color:blue\">three parameters</b> listed below:\n",
    "    <br> \n",
    "    <ol style='font-size:1.75rem;line-height:1.5'>\n",
    "        <li>Set <code>hessianThreshold</code> (used in SURF) to your desired value (bigger = less keypoints). \n",
    "            <br> The ideal value for the hessian threshhold is between <code>300</code> and <code>500</code>.</li>\n",
    "        <br>\n",
    "        <li>Set <code>nFeatures</code> (used in ORB) to set the maximum number of features found in the image.</li>\n",
    "        <br>\n",
    "        <li><code>MIN_MATCH_COUNT</code> determines the minimum number of matches for an object to be considered detected. We recommend that you set it to <code>20</code>.</li>\n",
    "    </ol>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Define hessianThreshold, nFeature, and MIN_MATCH_COUNT\n",
    "hessianThreshold = None  # SIFT\n",
    "nFeatures = None         # ORB\n",
    "MIN_MATCH_COUNT = None\n",
    "\n",
    "print('hessianThreshold: {}, nFeatures: {}, MIN_MATCH_COUNT: {}'.format(hessianThreshold, nFeatures, MIN_MATCH_COUNT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Detecting Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    The <b style=\"color:blue\">one way sign</b> below is the sign that we want to <b style=\"color:blue\">detect in the video stream.</b>\n",
    "    </p>\n",
    "    \n",
    "<img src=\"one_way.png\" alt=\"one_way\" style=\"width: 400px;\"/>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> As you've probably noticed, finding features between SIFT, SURF, and ORB got <b>kind of repetitive</b>. \n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    We've created <code>find_keypoints</code> function so that we can more easily find keypoints between the three algorithms. <b>Help us finish it!</b>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keypoints(img, feature_detection_algorithm):\n",
    "    ''' Find keypoints for image using selected feature detection algorithm.\n",
    "        Inputs: filename (string): RGB image filepath\n",
    "                feature_detection_algorithm (string): \"sift\", \"surf\", or \"orb\" \n",
    "        Outputs: keypoints (list)\n",
    "        '''\n",
    "    # TASK #1: Convert BGR image to grayscale via cv2.cvtColor\n",
    "    #          Save the grayscale image into the variable 'img_grayscale'\n",
    "    \n",
    "\n",
    "    # Shrink image to reduce keypoints for better run time\n",
    "    def shrink_image(image):\n",
    "        image = cv2.resize(image, None, fx=0.2, fy=0.2, interpolation=cv2.INTER_AREA)\n",
    "        return image\n",
    "    shrink_image(img)\n",
    "    shrink_image(img_grayscale)\n",
    "\n",
    "    # Create SIFT, SURF, or ORB objects\n",
    "    if feature_detection_algorithm=='sift':\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        # TASK #2: Detect SIFT keypoints and descriptors\n",
    "        \n",
    "        \n",
    "    elif feature_detection_algorithm=='surf':\n",
    "        surf = cv2.xfeatures2d.SURF_create(hessianThreshold, extended=True) \n",
    "        # TASK #3: Detect SURF keypoints and descriptors\n",
    "        \n",
    "        \n",
    "    elif feature_detection_algorithm=='orb':\n",
    "        orb = cv2.ORB_create(nFeatures)\n",
    "        # TASK #4: Detect ORB keypoints and descriptors\n",
    "        \n",
    "\n",
    "    # return keypoints and descriptors\n",
    "    return (keypoints, descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Let's <b>test out the function</b> we just created! <b>Make sure you run the cell block above.</b>\n",
    "    <ol style='font-size:1.75rem;line-height:2'>\n",
    "        <li><b style=\"color:blue\">Read</b> <code>one_way.png</code> as img, and use either <code>\"sift\"</code>, <code>\"surf\"</code>, or <code>\"orb\"</code> as the feature detection algorithm.</li>\n",
    "        <li>Play around and try to <b style=\"color:blue\">find the differences</b> between the three algorithms! </li>\n",
    "        <li><b style=\"color:blue\">Tweak</b> the <code>hessianThreshold</code>, <code>nFeatures</code>, and <code>MIN_MATCH_COUNT</code> parameters above for different results!</li>\n",
    "    </ol>\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    You should get something that looks like this:\n",
    "    </p>\n",
    "\n",
    "<img src=\"one_way_features.png\" alt=\"one_way_features\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK #1: Read \"one_way.png\" using cv2.imread. \n",
    "#          Save as 'img'. \n",
    "\n",
    "\n",
    "# TASK #2: Specify feature detection: \"sift\", \"surf\", \"orb\". \n",
    "#          Save as 'feature_detection_algorithm'.\n",
    "\n",
    "\n",
    "# Draw keypoints on image. Press 'ESC' to close window.\n",
    "keypoints, descriptors = find_keypoints(img, feature_detection_algorithm)\n",
    "cv2.drawKeypoints(img, keypoints, img, flags=5)\n",
    "cv2.imshow(\"{} keypoints\".format(feature_detection_algorithm), img)\n",
    "close_windows()\n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"# Keypoints: {}\".format(len(keypoints)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Matching Features\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Now we need to <b style=\"color:blue\">match the keypoints</b> from the one way sign with keypoints from the video frames. We can use use <code>flann.knnMatch</code> to <b style=\"color:blue\">define the number of matches</b> we want.\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p>\n",
    "    \n",
    "```python\n",
    "matches = flann.knnMatch(<description1>, <description2>, k=<num_best_matches>)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Complete the <code>calculate_matches</code> function below by using <code>flann.knnMatch</code>!\n",
    "    </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matches(des_img, des_frame, feature_detection_algorithm):\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    if feature_detection_algorithm==\"orb\":\n",
    "        FLANN_INDEX_LSH = 6\n",
    "        index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                           table_number = 6, \n",
    "                           key_size = 12,     \n",
    "                           multi_probe_level = 1)\n",
    "    else:\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        \n",
    "    # create FLANN object\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # if there are matches\n",
    "    if (des_img is not None) and (des_frame is not None):\n",
    "        # TASK #1: Use flann.knnMatch(). Save as \"matches\" \n",
    "        #          Use des_img for <description1>, des_frame for <description 2>\n",
    "        #          Use 2 for <num_best_matches>.\n",
    "        \n",
    "        \n",
    "    \n",
    "    # if there are NO matches   \n",
    "    else:\n",
    "        matches = []\n",
    "    \n",
    "    # return list of matches found\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Live Video Matching\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style=\"color:red\">Exercise:</b> \n",
    "    <br> Display the matches in our video!\n",
    "    <ol style='font-size:1.75rem;line-height:2'>\n",
    "        <li><b style=\"color:blue\">Specify</b> <code>img</code> and <code>feature_detection_algorithm</code> </li>\n",
    "        <li><b style=\"color:blue\">Run</b> the code block below</li>\n",
    "    </ol>\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    If all the functions were implemented correctly, you should get something that looks like this:\n",
    "    </p>\n",
    "\n",
    "<img src=\"one_way_matches.png\" alt=\"one_way_matches\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK #1: Read \"one_way.png\" using cv2.imread. \n",
    "#          Save as 'img'. \n",
    "\n",
    "\n",
    "# TASK #2: Specify feature detection to use: \"sift\", \"surf\", \"orb\". \n",
    "#          Save as 'feature_detection_algorithm'.\n",
    "\n",
    "\n",
    "def display_matches(frame):\n",
    "    # Find image keypoints \n",
    "    kp_img, des_img = find_keypoints(img, feature_detection_algorithm)\n",
    "    total_keypoints = float(len(kp_img))\n",
    "    \n",
    "    # find frame keypoints\n",
    "    frame = cv2.resize(frame, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_AREA)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    kp_frame, des_frame = find_keypoints(frame, feature_detection_algorithm)\n",
    "    \n",
    "    # calculate matches\n",
    "    matches = calculate_matches(des_img, des_frame, feature_detection_algorithm)\n",
    "        \n",
    "    # draw matches\n",
    "    img_matches, total_matches, m, good_matches = draw_matches(img, frame, total_keypoints, matches, kp_img, kp_frame)\n",
    "    \n",
    "    # homography to find object in frame\n",
    "    find_object(img_matches, img, total_matches, MIN_MATCH_COUNT, kp_img, kp_frame, m, good_matches, (0, 255, 0), img.shape[1])\n",
    "    \n",
    "    # show matches on video\n",
    "    cv2.imshow('video', img_matches)\n",
    "\n",
    "# TASK #3: Call video() with display_matches as input\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
